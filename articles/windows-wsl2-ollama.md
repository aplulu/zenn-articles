---
title: WSL2ä¸Šã§Ollamaã‚’ä½¿ã£ã¦ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’æ¨è«–å®Ÿè¡Œã™ã‚‹
emoji: "ğŸ¤–"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: [ "windows", "wsl", "ollama", "llm" ]
published: true
---

WSL2ä¸Šã§Ollamaã‚’ä½¿ã£ã¦ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’æ¨è«–å®Ÿè¡Œã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

# ã¯ã˜ã‚ã«

[Ollama](https://github.com/ollama/ollama)ã¯ã€LLMã‚’ä¸»ã«ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã®OSSãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚
ä»Šå›ã¯Ollamaã«ã‚ˆã‚‹LLMã®å®Ÿè¡Œç’°å¢ƒã‚’WSL2ä¸Šã«æ§‹ç¯‰ã—ã€Dockerä¸Šã§Ollamaã¨LLMã‚’å®Ÿè¡Œã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

åŒæ™‚ã«[open-webui](https://github.com/open-webui/open-webui)ã‚’ä½¿ç”¨ã—ã¦ã€ä¸€èˆ¬çš„ãªLLMã‚µãƒ¼ãƒ“ã‚¹ã¨åŒæ§˜ã«ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰æ°—è»½ã«LLMã‚’å®Ÿè¡Œã§ãã‚‹ç’°å¢ƒã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

ãŠã†ã¡ã«ã‚²ãƒ¼ãƒŸãƒ³ã‚°PCã‚„NVIDIA GPUã‚’æ­è¼‰ã—ãŸPCãŒã‚ã‚‹æ–¹ã«ãŠã™ã™ã‚ã§ã™ã€‚

# å‰æç’°å¢ƒ

* Windows 10 or 11
* WSL2
* NVIDIA GPU (VRAM 16GBä»¥ä¸Šæ¨å¥¨ã§ã™ã€‚)

# å®Ÿè¡Œæ‰‹é †

## WSL2ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

Microsoft Storeã‹ã‚‰Ubuntu 22.04.05 LTSã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

https://apps.microsoft.com/detail/9pn20msr04dw?hl=ja-JP&gl=JP

## Dockerã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ä»¥ä¸‹ã®æ‰‹é †ã«å¾“ã£ã¦Dockerã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚
https://docs.docker.com/engine/install/ubuntu/

```shell
# Uninstall old versions
$ for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done

# Add Docker's official GPG key:
$ sudo apt-get update
$ sudo apt-get install ca-certificates curl
$ sudo install -m 0755 -d /etc/apt/keyrings
$ sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
$ sudo chmod a+r /etc/apt/keyrings/docker.asc

# Add the repository to Apt sources:
$ echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
$ sudo apt-get update

# Install Docker
$ sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

# Start Docker
$ sudo systemctl enable docker
$ sudo systemctl enable containerd
$ sudo systemctl start docker
```

ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ã§Dockerã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«ç¶šã‘ã¦ã€ä¸‹è¨˜ã®æ‰‹é †ã«å¾“ã£ã¦è¨­å®šã—ã¾ã™ã€‚

https://docs.docker.com/engine/install/linux-postinstall/

```shell
# Create the docker group.
$ sudo groupadd docker

# Add your user to the docker group.
$ sudo usermod -aG docker $USER
$ newgrp docker
```

## NVIDIA Container Toolkitã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

æ¬¡ã®æ‰‹é †ã«å¾“ã£ã¦NVIDIA Container Toolkitã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html

```shell
# Add the package repositories
$ curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

# Update the package repositories
$ sudo apt-get update

# Install the NVIDIA Container Toolkit
$ sudo apt-get install -y nvidia-container-toolkit

# Configure the runtime
$ sudo nvidia-ctk runtime configure --runtime=docker

# Restart Docker
$ sudo systemctl restart docker
```

ã“ã“ã¾ã§å®Œäº†ã™ã‚‹ã¨ã€WSL2ä¸Šã®Dockerã‹ã‚‰NVIDIA GPUã‚’åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§GPUãŒåˆ©ç”¨ã§ãã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚

```shell
$ docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi
```

æˆåŠŸã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå‡ºåŠ›ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

```
aplulu@ragdoll:~$ docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi
Mon Dec 30 08:46:10 2024
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.36                 Driver Version: 546.33       CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3090        On  | 00000000:01:00.0  On |                  N/A |
| 44%   55C    P3              85W / 350W |   3297MiB / 24576MiB |     31%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A        25      G   /Xwayland                                 N/A      |
+---------------------------------------------------------------------------------------+
```

## Ollamaã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

é©å½“ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«Ollamaç”¨ãƒœãƒªãƒ¥ãƒ¼ãƒ ãªã©ã‚’æ ¼ç´ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã€Docker Composeã®å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚

Ollamaç”¨ã®ãƒœãƒªãƒ¥ãƒ¼ãƒ ã«ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ãªã©ãŒä¿å­˜ã•ã‚Œã¦ã‚‹ãŸã‚ã€æ•°åGBç¨‹åº¦ã®å®¹é‡ãŒå¿…è¦ã§ã™ã€‚

```shell
$ mkdir -p ~/ollama
$ cd ~/ollama
$ cat <<EOF > compose.yaml
services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ./ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              driver: nvidia
              count: all
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "8080:8080"
    environment:
      API_URL: http://ollama:11434
      # ä»Šå›ã¯ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã€èªè¨¼ã‚’ç„¡åŠ¹åŒ–ã—ã¦ã„ã¾ã™ã€‚
      WEBUI_AUTH: "False"
EOF
```

æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§Ollamaã¨open-webuiã‚’èµ·å‹•ã—ã¾ã™ã€‚

```shell
$ docker compose up -d
```

## ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

Ollamaã§æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚
ä»Šå›ã¯MicrosoftãŒé–‹ç™ºã—ãŸphi3ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
ãŠä½¿ã„ã®GPUã®VRAMã«åˆã‚ã›ã¦ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

* 14Bãƒ¢ãƒ‡ãƒ« (æ¨å¥¨: VRAM 16GBä»¥ä¸Š)
  * https://ollama.com/library/phi3:14b-medium-4k-instruct-q6_K
* 3.8Bãƒ¢ãƒ‡ãƒ« (æ¨å¥¨: VRAM 8GBä»¥ä¸Š)
  * https://ollama.com/library/phi3:3.8b-mini-4k-instruct-q6_K

```shell
# 14Bãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
$ docker compose exec ollama ollama pull phi3:14b-medium-4k-instruct-q6_K

# 3.8Bãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
$ docker compose exec ollama ollama pull phi3:3.8b-mini-4k-instruct-q6_K
```

## æ¨è«–å®Ÿè¡Œãƒ†ã‚¹ãƒˆ

ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ãŸã‚‰ã€curlã§æ¨è«–ã‚’å®Ÿè¡Œã—ã¦ã¿ã¾ã™ã€‚

```shell
# 14Bãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ã‚’å®Ÿè¡Œ
$ curl -v http://localhost:11434/api/chat \
  -d '{"model":"phi3:14b-medium-4k-instruct-q6_K","stream":false,"messages":[{"role":"user","content":"What famous constellations are visible in December?"}]}'
# 3.8Bãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ã‚’å®Ÿè¡Œ
$ curl -v http://localhost:11434/api/chat \
  -d '{"model":"phi3:3.8b-mini-4k-instruct-q6_K","stream":false,"messages":[{"role":"user","content":"What famous constellations are visible in December?"}]}'
```

ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒè¿”ã£ã¦ãã‚Œã°æˆåŠŸã§ã™ã€‚åˆå›ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ã€æ•°åç§’ç¨‹åº¦ã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

```
aplulu@ragdoll:/mnt/d/ollama$ curl -v http://localhost:11434/api/chat -d '{"model":"phi3:14b-medium-4k-instruct-q6_K","stream":false,"messages":[{"role":"user","content":"What famous constellations are visible in December?"}]}'
*   Trying 127.0.0.1:11434...
* Connected to localhost (127.0.0.1) port 11434 (#0)
> POST /api/chat HTTP/1.1
> Host: localhost:11434
> User-Agent: curl/7.81.0
> Accept: */*
> Content-Length: 152
> Content-Type: application/x-www-form-urlencoded
>
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Content-Type: application/json; charset=utf-8
< Date: Tue, 31 Dec 2024 07:57:23 GMT
< Content-Length: 830
<
* Connection #0 to host localhost left intact
{"model":"phi3:14b-medium-4k-instruct-q6_K","created_at":"2024-12-31T07:57:23.905011899Z","message":{"role":"assistant","content":"In December, some of the most prominent constellations that can be seen from the Northern Hemisphere include Orion, with its distinctive three-star belt; Taurus, marked by the bright star Aldebaran and the Pleiades cluster (also known as the Seven Sisters); Gemini, featuring the twin stars Castor and Pollux; and Canis Major, home to Sirius, the brightest star in our night sky. In the Southern Hemisphere, constellations like Crux (the Southern Cross) are visible prominently during this time of year.\n\n-----"},"done_reason":"stop","done":true,"total_duration":4597126661,"load_duration":98263985,"prompt_eval_count":19,"prompt_eval_duration":7000000,"eval_count":131,"eval_duration":4490000000}
```

## open-webui

open-webuiã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰æ°—è»½ã«LLMã®æ¨è«–ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

http://localhost:8080 ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã€è‰²ã€…ã¨è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚

![open-webui](/images/windows-wsl2-ollama/open-webui.png)

# è£œè¶³æƒ…å ±

## ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

phi3ä»¥å¤–ã«ã‚‚æ§˜ã€…ãªãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚

ã©ã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒæä¾›ã•ã‚Œã¦ã„ã‚‹ã‹ã¯ä»¥ä¸‹ã®URLã‹ã‚‰ç¢ºèªã§ãã¾ã™ã€‚

https://ollama.com/library

ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹éš›ã¯ã€`ollama pull`ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```shell
$ docker compose exec ollama ollama pull [ãƒ¢ãƒ‡ãƒ«å]
```

## ç°¡æ˜“ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

ãƒˆãƒ¼ã‚¯ãƒ³å‡ºåŠ›é€Ÿåº¦ã‚’èª¿ã¹ã‚‹ã«ã¯ `--verbose` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä»˜ã‘ã¦å®Ÿè¡Œã™ã‚‹ã“ã¨ã§è¡¨ç¤ºã™ã‚‹ã“ã¨ãŒå‡ºæ¥ã¾ã™ã€‚

```shell
$ docker compose exec ollama ollama run phi3:14b-medium-4k-instruct-q6_K --verbose "æ—¥æœ¬ã§12æœˆã«è¦‹ãˆã‚‹æœ‰åãªæ˜Ÿåº§ã§çŸ­ã„ãŠè©±ã‚’åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚"
```

https://www.youtube.com/watch?v=LjO50mdU0O8

ã‚ãŸã—ã®ç’°å¢ƒã®RTX 3090ã§phi3:14b-medium-4k-instruct-q6_Kãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸå ´åˆã€47.91ãƒˆãƒ¼ã‚¯ãƒ³/ç§’ç¨‹åº¦ã®ãƒˆãƒ¼ã‚¯ãƒ³å‡ºåŠ›æ€§èƒ½ãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚

## LANå†…ã®ä»–ã®ãƒ‡ãƒã‚¤ã‚¹ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ (Windows 10)

Windows 10ã®ç’°å¢ƒã§LANå†…ã®ä»–ã®ãƒ‡ãƒã‚¤ã‚¹ã‹ã‚‰Ollamaã‚„open-webuiã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã«ã¯ã€ãƒãƒ¼ãƒˆãƒ—ãƒ­ã‚­ã‚·ãƒ¼ã‚’è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

```shell
# Ollamaã®ãƒãƒ¼ãƒˆ11434ã‚’è»¢é€
netsh interface portproxy add v4tov4 listenport=11434 listenaddress=0.0.0.0 connectport=11434 connectaddress=(wsl hostname -I)
# open-webuiã®ãƒãƒ¼ãƒˆ8080ã‚’è»¢é€
netsh interface portproxy add v4tov4 listenport=8080 listenaddress=0.0.0.0 connectport=8080 connectaddress=(wsl hostname -I)
``` 

è©³ç´°ã«ã¤ã„ã¦ã¯ä»¥ä¸‹ã®URLã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
https://learn.microsoft.com/ja-jp/windows-server/networking/technologies/netsh/netsh-interface-portproxy

## LANå†…ã®ä»–ã®ãƒ‡ãƒã‚¤ã‚¹ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ (WSL2ã®ãƒŸãƒ©ãƒ¼ãƒ¢ãƒ¼ãƒ‰ Windows 11 22H2ä»¥é™)

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯WSL2ã¯NATãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¦ã„ã‚‹ãŸã‚ã€LANå†…ã®ä»–ã®ãƒ‡ãƒã‚¤ã‚¹ã‹ã‚‰Ollamaã‚„open-webuiã«ã¯ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚
WSL2ã‹ã‚‰å°å…¥ã•ã‚ŒãŸãƒŸãƒ©ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€LANå†…ã®ä»–ã®ãƒ‡ãƒã‚¤ã‚¹ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

ãƒŸãƒ©ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã€ãƒ›ãƒ¼ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« `.wslconfig` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€ä»¥ä¸‹ã®ã‚ˆã†ã«è¨˜è¿°ã—ã¾ã™ã€‚

```shell
$ cd /mnt/c/Users/[ãƒ¦ãƒ¼ã‚¶å]
$ cat <<EOF > .wslconfig
[wsl2]
networkingMode=mirrored 
EOF
```

`.wslconfig` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ãŸã‚‰ã€WSL2ã‚’å†èµ·å‹•ã—ã¾ã™ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã¯PowerShellã§å®Ÿè¡Œã—ã¾ã™ã€‚

```shell
wsl --shutdown
```

ç¶šã„ã¦Hyper-Vãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ã®ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ ã—ã€ãƒãƒ¼ãƒˆ11434ã¨8080ã‚’é–‹æ”¾ã—ã¾ã™ã€‚
```shell
# Ollamaã®ãƒãƒ¼ãƒˆ11434ã‚’é–‹æ”¾
New-NetFirewallHyperVRule -Name MyWebServer -DisplayName "Ollama" -Direction Inbound -VMCreatorId '{40E0AC32-46A5-438A-A0B2-2B479E8F2E90}' -Protocol TCP -LocalPorts 11434

# open-webuiã®ãƒãƒ¼ãƒˆ8080ã‚’é–‹æ”¾
New-NetFirewallHyperVRule -Name MyWebServer -DisplayName "open-webui" -Direction Inbound -VMCreatorId '{40E0AC32-46A5-438A-A0B2-2B479E8F2E90}' -Protocol TCP -LocalPorts 8080
```

è©³ç´°ã¯ä»¥ä¸‹ã®URLã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

https://learn.microsoft.com/ja-jp/windows/security/operating-system-security/network-security/windows-firewall/hyper-v-firewall

# ãŠã‚ã‚Šã«

ã“ã“ã¾ã§ã§ã€ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§LLMã‚’å‹•ã‹ã™æº–å‚™ãŒæ•´ã„ã¾ã—ãŸï¼
open-webuiã‚’ä½¿ã†ã“ã¨ã§ã€ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ç°¡å˜ã«LLMã¨å¯¾è©±ã§ãã¾ã™ã®ã§ã€ãœã²è‰²ã€…ã¨è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚
